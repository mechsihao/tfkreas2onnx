# tf/keras模型线上推理加速方法： 
## 1.tf2onnx转换tf/keras模型至onnx格式，用onnxruntime来加速在线推理
### freeze转换方法使用
 - 

## 3.将tf/keras参数冻结，tf加载静态图推理
#### 压测实验效果：
用上面的两种方法将一个4层bert模型分别转换后压测实验，实验方法是用二者分别对相同的1w条query进行推理，查看压测时间，效果如下：
| 优化方法  | P90/s | P95/s | P99/s | P999/s | 
| --- | :---: | :---: | :---: | :---: |
| onnxruntime | 0.00549 | 0.00672 | 0.00932 | 0.07780 | 
| freeze参数 | 0.00349 | 0.00373 | 0.00439 | 0.00897 | 
#### 结论： 
    - 从压测来看，效果好像是freeze参数更好一些
    - 二者相同点是大部分推理耗时都在2.5ms以内
    - 但onnxruntime有个优点，有75%的推理耗时在3ms以内，而 freeze参数 则是只有60%的推理耗时在3ms以内
    - 个人推测onnxruntime效果差的原因可能是笔者用的cpu比较垃圾，导致前面的计算阻塞了，从而后面的计算堆积了起来。在真实场景下效果应该会比freeze参数稳定。